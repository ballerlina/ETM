{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty = fetch_20newsgroups(subset='all', shuffle=False, remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 18846\n",
      "Number of different categories: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of articles: ' + str(len(twenty.data)))\n",
    "print('Number of different categories: ' + str(len(twenty.target_names)))\n",
    "twenty.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_targets = set()\n",
    "rare_targets = set()\n",
    "select_target_names = set(twenty.target_names)\n",
    "rare_target_names = set()\n",
    "#select_target_names = {'rec.autos', 'alt.atheism', 'sci.med', 'comp.sys.ibm.pc.hardware'}\n",
    "rare_target_names = {'rec.autos', 'rec.motorcycles'}\n",
    "\n",
    "for i, name in enumerate(twenty.target_names):\n",
    "    if name in rare_target_names:\n",
    "        rare_targets.add(i)\n",
    "    if name in select_target_names:\n",
    "        select_targets.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_grouped = defaultdict(list)\n",
    "\n",
    "for i, article in enumerate(twenty.data):\n",
    "    group_num = twenty.target[i]\n",
    "    if group_num in select_targets:\n",
    "        twenty_grouped[group_num].append((group_num, article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "963\n",
      "991\n",
      "799\n",
      "988\n",
      "990\n",
      "984\n",
      "940\n",
      "999\n",
      "975\n",
      "198\n",
      "985\n",
      "199\n",
      "987\n",
      "973\n",
      "982\n",
      "910\n",
      "775\n",
      "628\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "for k in twenty_grouped.keys():\n",
    "    if k in rare_targets:\n",
    "        twenty_grouped[k] = twenty_grouped[k][: int(len(twenty_grouped[k]) * 0.2)]\n",
    "    print(len(twenty_grouped[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split equally by group; returns (group index, data) pair\n",
    "def tr_va_ts_split(grouped, tr_prop, va_prop, ts_prop):\n",
    "    assert tr_prop + va_prop + ts_prop == 1.\n",
    "    train, valid, test = list(), list(), list()\n",
    "    for i in range(len(grouped.keys())):\n",
    "        num_tr = int(tr_prop * len(grouped[i]))\n",
    "        num_va = int(va_prop * len(grouped[i]))\n",
    "        train.extend(grouped[i][: num_tr])\n",
    "        valid.extend(grouped[i][num_tr : (num_tr + num_va)])\n",
    "        test.extend(grouped[i][(num_tr + num_va) :])\n",
    "    random.Random(5).shuffle(train)\n",
    "    random.Random(5).shuffle(valid)\n",
    "    random.Random(5).shuffle(test)\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of weird params: train/valid split (0.1 for 7 topics, 0.05 else for va), and min_df=50 vs. min_df=10 vs. min_df=30 change for 7 topics vs. 4 topics vs. rare topic (all these same for comparisons within a type of test, so should be fine)\n",
    "#train, valid, test = tr_va_ts_split(twenty_grouped, 0.60, 0.1, 0.3)\n",
    "train, valid, test = tr_va_ts_split(twenty_grouped, 0.65, 0.05, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11209\n",
      "852\n",
      "5196\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "                          use_idf=False,\n",
    "                          norm=None,\n",
    "                          token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\")\n",
    "\n",
    "# drop docs that don't have at least min_cnt words (can only check after tfidf transform)\n",
    "def split_and_drop(mat, labels, min_cnt=10, drop=True, verbose=True):\n",
    "    counts = np.asarray(np.split(mat.data.astype(np.uint8), mat.indptr[1:-1]))\n",
    "    tokens = np.asarray(np.split(mat.indices.astype(np.uint16), mat.indptr[1:-1]))\n",
    "    small_idxs = []\n",
    "    if drop:\n",
    "        for i in range(len(counts)):\n",
    "            if counts[i].sum() < min_cnt:\n",
    "                small_idxs.append(i)\n",
    "        if verbose:\n",
    "            print(f'Deleted {len(small_idxs)} docs with <{min_cnt} words')\n",
    "    return np.delete(counts, small_idxs), np.delete(tokens, small_idxs), np.delete(labels, small_idxs), small_idxs\n",
    "\n",
    "def split_and_drop_mult(mats, labels, min_cnt=10, verbose=True):\n",
    "    counts_list, tokens_list = [], []\n",
    "    small_idxs = set()\n",
    "    for j, mat in enumerate(mats):\n",
    "        if j > 0:\n",
    "            min_cnt = 1\n",
    "        counts = np.asarray(np.split(mat.data.astype(np.uint8), mat.indptr[1:-1]))\n",
    "        tokens = np.asarray(np.split(mat.indices.astype(np.uint16), mat.indptr[1:-1]))\n",
    "        counts_list.append(counts)\n",
    "        tokens_list.append(tokens)\n",
    "        for i in range(len(counts)):\n",
    "            if counts[i].sum() < min_cnt:\n",
    "                small_idxs.add(i)\n",
    "    if verbose:\n",
    "        print(f'Deleted {len(small_idxs)} docs with <{min_cnt} words')\n",
    "    small_idxs = list(small_idxs)\n",
    "    for i in range(len(mats)):\n",
    "        counts_list[i] = np.delete(counts_list[i], small_idxs)\n",
    "        tokens_list[i] = np.delete(tokens_list[i], small_idxs)\n",
    "    labels = np.delete(labels, small_idxs)\n",
    "    return counts_list, tokens_list, labels, small_idxs\n",
    "\n",
    "def process(train, valid, test):\n",
    "    tr_labels, tr_data = [list(t) for t in zip(*train)]\n",
    "    va_labels, va_data = [list(t) for t in zip(*valid)]\n",
    "    ts_labels, ts_data = [list(t) for t in zip(*test)]\n",
    "    \n",
    "    tf_vect.set_params(min_df=30, max_df=0.7, vocabulary=None)\n",
    "    tr_mat = tf_vect.fit_transform(tr_data).sorted_indices()\n",
    "    vocab = tf_vect.get_feature_names()\n",
    "    \n",
    "    tf_vect.set_params(min_df=1, max_df=1., vocabulary=vocab)\n",
    "    vocab2 = tf_vect.get_feature_names()\n",
    "    va_mat = tf_vect.fit_transform(va_data).sorted_indices()\n",
    "    ts_mat = tf_vect.fit_transform(ts_data).sorted_indices()\n",
    "    \n",
    "    tr_counts, tr_tokens, tr_labels, _ = split_and_drop(tr_mat, tr_labels)\n",
    "    va_counts, va_tokens, va_labels, _ = split_and_drop(va_mat, va_labels)\n",
    "    \n",
    "    ts_clean_data = ts_data\n",
    "    ts_h1_data = [article[: len(article) // 2] for article in ts_clean_data]\n",
    "    ts_h2_data = [article[len(article) // 2 :] for article in ts_clean_data]\n",
    "    ts_h1_mat = tf_vect.fit_transform(ts_h1_data).sorted_indices()\n",
    "    ts_h2_mat = tf_vect.fit_transform(ts_h2_data).sorted_indices()\n",
    "    ts_counts, ts_tokens, ts_labels, _ = split_and_drop_mult([ts_mat, ts_h1_mat, ts_h2_mat], ts_labels)\n",
    "    counts = [tr_counts, va_counts] + ts_counts\n",
    "    tokens = [tr_tokens, va_tokens] + ts_tokens\n",
    "    return counts, tokens, [tr_labels, va_labels, ts_labels], vocab\n",
    "    \n",
    "def save(counts, tokens, labels, vocab, path, prefix):\n",
    "    with open(os.path.join(path, 'vocab.pkl'), 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    with open(os.path.join(path, 'labels.pkl'), 'wb') as f:\n",
    "        pickle.dump({'train': labels[0], 'valid': labels[1], 'test': labels[2]}, f)\n",
    "    for i, name in enumerate(['tr', 'va', 'ts', 'ts_h1', 'ts_h2']):\n",
    "        scipy.io.savemat(os.path.join(path, f'{prefix}_{name}_counts.mat'), {'counts': counts[i]})\n",
    "        scipy.io.savemat(os.path.join(path, f'{prefix}_{name}_tokens.mat'), {'tokens': tokens[i]})\n",
    "    print('Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 465 docs with <10 words\n",
      "Deleted 36 docs with <10 words\n",
      "Deleted 245 docs with <1 words\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "data_path = './../data/my_20ng_rare'\n",
    "counts, tokens, labels, vocab = process(train, valid, test)\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "save(counts, tokens, labels, vocab, data_path, 'bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train articles: 10744\n",
      "Num valid articles: 816\n",
      "Num test articles:  4951\n",
      "Vocab size: 5350\n"
     ]
    }
   ],
   "source": [
    "print(f'Num train articles: {len(counts[0])}')\n",
    "print(f'Num valid articles: {len(counts[1])}')\n",
    "print(f'Num test articles:  {len(counts[2])}')\n",
    "print(f'Vocab size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
