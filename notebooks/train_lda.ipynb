{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import torch\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from etm import ETM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './../data/my_20ng_2'\n",
    "\n",
    "def load(path, prefix):\n",
    "    return scipy.io.loadmat(os.path.join(path, f'{prefix}_counts.mat'))['counts'].squeeze(), scipy.io.loadmat(os.path.join(path, f'{prefix}_tokens.mat'))['tokens'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csr(counts, tokens): \n",
    "    indptr = np.zeros(len(tokens)+1, dtype=np.uint32)\n",
    "    for i in range(len(tokens)):\n",
    "        indptr[i+1] = len(tokens[i].squeeze()) + indptr[i]\n",
    "    tokens_flat, counts_flat = [], []\n",
    "    for i in range(len(tokens)):\n",
    "        doc_tokens = tokens[i].squeeze()\n",
    "        doc_counts = counts[i].squeeze()\n",
    "        tokens_flat.extend(doc_tokens.tolist())\n",
    "        counts_flat.extend(doc_counts.tolist())\n",
    "    return scipy.sparse.csr_matrix((np.array(counts_flat), np.array(tokens_flat), indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(data_path, 'vocab.pkl'), 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114, 1901)\n"
     ]
    }
   ],
   "source": [
    "train_mat = get_csr(*load(data_path, 'bow_tr'))\n",
    "X = train_mat.todense()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10, perplexity: 915.7583\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "lda = LatentDirichletAllocation(n_components=k,\n",
    "                                learning_method='online',\n",
    "                                learning_decay=0.85,\n",
    "                                learning_offset=10.,\n",
    "                                evaluate_every=10,\n",
    "                                verbose=1,\n",
    "                                random_state=5).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './../results'\n",
    "with open(os.path.join(save_path, f'lda_{k}_{os.path.basename(data_path)}.pkl'), 'wb') as f:\n",
    "    pickle.dump(lda, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(dists, n_top_words):\n",
    "    topics = []\n",
    "    for dist in dists:\n",
    "        top_word_idxs = np.argsort(dist)[::-1][:n_top_words]\n",
    "        topics.append([vocab[i] for i in top_word_idxs])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ['god', 'one', 'people', 'edu', 'writes', 'would', 'think', 'article', 'atheism', 'say', 'believe', 'like', 'religion', 'must', 'system', 'atheists', 'well', 'many', 'know', 'something']\n",
      "\n",
      "[2] ['drive', 'scsi', 'controller', 'card', 'ide', 'system', 'one', 'bus', 'disk', 'drives', 'get', 'use', 'would', 'hard', 'pc', 'edu', 'know', 'like', 'problem', 'also']\n",
      "\n",
      "[3] ['com', 'jesus', 'would', 'one', 'edu', 'writes', 'article', 'know', 'matthew', 'people', 'said', 'time', 'like', 'could', 'think', 'see', 'john', 'tek', 'really', 'vice']\n",
      "\n",
      "[4] ['edu', 'com', 'people', 'writes', 'time', 'article', 'book', 'jesus', 'liar', 'would', 'christian', 'os', 'first', 'read', 'one', 'comp', 'ca', 'david', 'saturn', 'wwc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = get_topic_words(lda.components_, 20)\n",
    "for i, t in enumerate(topics):\n",
    "    print(f'[{i+1}] {t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(data_path, mode='test'):\n",
    "    if mode == 'test':\n",
    "        prefix = 'ts'\n",
    "    elif mode == 'train':\n",
    "        prefix = 'tr'\n",
    "    counts, tokens = load(data_path, f'bow_{prefix}')\n",
    "    test_mat = get_csr(counts, tokens)\n",
    "    X = test_mat.todense()\n",
    "    with open(os.path.join(data_path, 'labels.pkl'), 'rb') as f:\n",
    "        labels = pickle.load(f)[mode]\n",
    "    with open(os.path.join(data_path, 'vocab.pkl'), 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    print(X.shape)\n",
    "    return X, counts, tokens, labels, vocab\n",
    "\n",
    "# Squish all the sub-categories together (on the full dataset)\n",
    "def collect_labels(labels):\n",
    "    new_labels = []\n",
    "    for label in labels:\n",
    "        if label == 0:\n",
    "            new_labels.append(0)  # religion\n",
    "        elif label <= 5:\n",
    "            new_labels.append(1)  # computers\n",
    "        elif label == 6:\n",
    "            new_labels.append(2)  # sale\n",
    "        elif label <= 8:\n",
    "            new_labels.append(3)  # cars\n",
    "        elif label <= 10:\n",
    "            new_labels.append(4)  # sports\n",
    "        elif label <= 14:\n",
    "            new_labels.append(5)  # science\n",
    "        elif label == 15:\n",
    "            new_labels.append(0)\n",
    "        elif label <= 17:\n",
    "            new_labels.append(6)  # politics\n",
    "        else:\n",
    "            new_labels.append(0)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "def lda_doc_topic(model_path, X):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        lda_model = pickle.load(f)\n",
    "    return lda_model.transform(X)\n",
    "    #return doc_topic_dists.argmax(axis=1)\n",
    "    \n",
    " # ETM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def etm_doc_topic(model_path, counts, tokens, vocab, truncate_first=None):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        etm_model = torch.load(f)\n",
    "    etm_model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_data = data.get_batch(tokens, counts, range(len(counts)), len(vocab), device)\n",
    "        all_data_norm = all_data / (all_data.sum(1).unsqueeze(1))\n",
    "        thetas, _ = etm_model.get_theta(all_data_norm)\n",
    "    if truncate_first is not None:\n",
    "        return thetas.numpy()[:, 7:]\n",
    "        #return thetas.numpy()[:, 7:].argmax(axis=1)\n",
    "    else:\n",
    "        return thetas.numpy()\n",
    "        #return thetas.numpy().argmax(axis=1)\n",
    "        \n",
    "def cluster(thetas):\n",
    "    return thetas.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_path1 = '../results/lda_7_my_20ng.pkl'\n",
    "lda_path2 = '../results/lda_4_my_20ng_2.pkl'\n",
    "lda_path3 = '../results/lda_20_my_20ng_rare.pkl'\n",
    "etm_path1 = '../results/etm_20ng_K_7_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0'\n",
    "etm_path2 = '../results/etm_20ng_K_7_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0_computers_cars_sports_science_sale_politics_religion_SeedLd_1.0'\n",
    "etm_path3 = '../results/etm_20ng_K_7_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0_computers_cars_sports_science_sale_politics_religion_SeedLd_0.1'\n",
    "etm_path4 = '../results/etm_20ng_K_14_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0_computers_cars_sports_science_sale_politics_religion_SeedLd_1.0'\n",
    "etm_path5 = '../results/etm_20ng_K_4_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0'\n",
    "etm_path6 = '../results/etm_20ng_K_4_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0_cars_religion_science_hardware_SeedLd_1.0'\n",
    "etm_path7 = '../results/etm_20ng_rare_K_20_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0'\n",
    "etm_path8 = '../results/etm_20ng_rare_K_20_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_0_cars_SeedLd_1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very slow n^2 operation...\n",
    "def confusion(y, yhat):\n",
    "    cf = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "    for i in range(len(y)):\n",
    "        for j in range(i+1, len(y)):\n",
    "            if y[i] != y[j] and yhat[i] != yhat[j]:\n",
    "                cf['tn'] += 1\n",
    "            elif y[i] != y[j] and yhat [i] == yhat[j]:\n",
    "                cf['fp'] += 1\n",
    "            elif y[i] == y[j] and yhat[i] != yhat[j]:\n",
    "                cf['fn'] += 1\n",
    "            else:\n",
    "                cf['tp'] += 1\n",
    "    print(cf)\n",
    "    return cf\n",
    "\n",
    "def f1_score(cf):\n",
    "    precision = cf['tp'] / (cf['tp'] + cf['fp'])\n",
    "    recall = cf['tp'] / (cf['tp'] + cf['fn'])\n",
    "    F = 2 * (precision * recall) / (precision + recall)\n",
    "    print(f'Precision: {round(precision, 4)}, recall: {round(recall, 4)}, F measure: {round(F, 4)}')\n",
    "    return precision, recall, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5379, 3455)\n",
      "5379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 1044484, 'fp': 2201984, 'tn': 9724623, 'fn': 1493040}\n",
      "Precision: 0.3217, recall: 0.4116, F measure: 0.3612\n",
      "{'tp': 993954, 'fp': 1436480, 'tn': 10490127, 'fn': 1543570}\n",
      "Precision: 0.409, recall: 0.3917, F measure: 0.4001\n",
      "{'tp': 1937099, 'fp': 8906576, 'tn': 3020031, 'fn': 600425}\n",
      "Precision: 0.1786, recall: 0.7634, F measure: 0.2895\n"
     ]
    }
   ],
   "source": [
    "# 7 topics\n",
    "X_test_7, test_counts_7, test_tokens_7, labels_7, vocab_7 = get('./../data/my_20ng', mode='test')\n",
    "labels_7 = collect_labels(labels_7)\n",
    "\n",
    "lda_dt1 = lda_doc_topic(lda_path1, X_test_7)\n",
    "etm_dt1 = etm_doc_topic(etm_path1, test_counts_7, test_tokens_7, vocab_7)\n",
    "etm_dt2 = etm_doc_topic(etm_path2, test_counts_7, test_tokens_7, vocab_7)\n",
    "clust_lda1 = cluster(lda_dt1)\n",
    "clust_etm1 = cluster(etm_dt1)\n",
    "clust_etm2 = cluster(etm_dt2)\n",
    "\n",
    "cf_lda1 = confusion(labels_7, clust_lda1)\n",
    "_ = f1_score(cf_lda1)\n",
    "cf_etm1 = confusion(labels_7, clust_etm1)\n",
    "_ = f1_score(cf_etm1)\n",
    "cf_etm2 = confusion(labels_7, clust_etm2)\n",
    "_ = f1_score(cf_etm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 1901)\n",
      "519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 53161, 'fp': 624, 'tn': 66116, 'fn': 14520}\n",
      "Precision: 0.9884, recall: 0.7855, F measure: 0.8753\n",
      "{'tp': 37829, 'fp': 4610, 'tn': 62130, 'fn': 29852}\n",
      "Precision: 0.8914, recall: 0.5589, F measure: 0.6871\n",
      "{'tp': 57409, 'fp': 3448, 'tn': 63292, 'fn': 10272}\n",
      "Precision: 0.9433, recall: 0.8482, F measure: 0.8933\n"
     ]
    }
   ],
   "source": [
    "# 4 topics\n",
    "X_test_4, test_counts_4, test_tokens_4, labels_4, vocab_4 = get('./../data/my_20ng_2', mode='test')\n",
    "\n",
    "lda_dt2 = lda_doc_topic(lda_path2, X_test_4)\n",
    "etm_dt5 = etm_doc_topic(etm_path5, test_counts_4, test_tokens_4, vocab_4)\n",
    "etm_dt6 = etm_doc_topic(etm_path6, test_counts_4, test_tokens_4, vocab_4)\n",
    "clust_lda2 = cluster(lda_dt2)\n",
    "clust_etm5 = cluster(etm_dt5)\n",
    "clust_etm6 = cluster(etm_dt6)\n",
    "\n",
    "cf_lda2 = confusion(labels_4, clust_lda2)\n",
    "_ = f1_score(cf_lda2)\n",
    "cf_etm5 = confusion(labels_4, clust_etm5)\n",
    "_ = f1_score(cf_etm5)\n",
    "cf_etm6 = confusion(labels_4, clust_etm6)\n",
    "_ = f1_score(cf_etm6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10744, 5350)\n",
      "(4951, 5350)\n"
     ]
    }
   ],
   "source": [
    "X_train, train_counts, train_tokens, train_labels, vocab = get('./../data/my_20ng_rare', mode='train')\n",
    "X_test, test_counts, test_tokens, test_labels, vocab = get('./../data/my_20ng_rare', mode='test')\n",
    "train_labels = collect_labels(train_labels)\n",
    "test_labels = collect_labels(test_labels)\n",
    "rare_idxs = list(np.argwhere(np.array(test_labels) == 3).squeeze())  # cars only\n",
    "test_labels_rare = [test_labels[i] for i in rare_idxs]\n",
    "logit_params = {'solver': 'liblinear', 'multi_class': 'ovr', 'class_weight': 'balanced'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dt_train = lda_doc_topic(lda_path3, X_train)\n",
    "lda_dt_test = lda_doc_topic(lda_path3, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7612603514441527\n"
     ]
    }
   ],
   "source": [
    "logit_lda = LogisticRegression(**logit_params).fit(lda_dt_train, train_labels)\n",
    "print(logit_lda.score(lda_dt_test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "etm_dt_train1 = etm_doc_topic(etm_path7, train_counts, train_tokens, vocab)\n",
    "etm_dt_test1 = etm_doc_topic(etm_path7, test_counts, test_tokens, vocab)\n",
    "etm_dt_train2 = etm_doc_topic(etm_path8, train_counts, train_tokens, vocab)\n",
    "etm_dt_test2 = etm_doc_topic(etm_path8, test_counts, test_tokens, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927893354877802\n",
      "0.6802666128054938\n"
     ]
    }
   ],
   "source": [
    "logit_etm1 = LogisticRegression(**logit_params).fit(etm_dt_train1, train_labels)\n",
    "logit_etm2 = LogisticRegression(**logit_params).fit(etm_dt_train2, train_labels)\n",
    "print(logit_etm1.score(etm_dt_test1, test_labels))\n",
    "print(logit_etm2.score(etm_dt_test2, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6460176991150443"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_dt_test_rare = lda_doc_topic(lda_path3, X_test[rare_idxs])\n",
    "logit_lda.score(lda_dt_test_rare, test_labels_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/serialization.py:493: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5663716814159292\n",
      "0.6637168141592921\n"
     ]
    }
   ],
   "source": [
    "etm_dt_test_rare1 = etm_doc_topic(etm_path7, test_counts[rare_idxs], test_tokens[rare_idxs], vocab)\n",
    "etm_dt_test_rare2 = etm_doc_topic(etm_path8, test_counts[rare_idxs], test_tokens[rare_idxs], vocab)\n",
    "print(logit_etm1.score(etm_dt_test_rare1, test_labels_rare))\n",
    "print(logit_etm2.score(etm_dt_test_rare2, test_labels_rare))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words_etm(model_path, num_topics=4, num_words=10):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = torch.load(f)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        beta = model.get_beta()\n",
    "        topic_indices = list(np.random.choice(num_topics, 10)) # 10 random topics\n",
    "        for k in range(num_topics):#topic_indices:\n",
    "            gamma = beta[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            print('Topic {}: {}'.format(k, topic_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['scsi', 'drive', 'system', 'drives', 'controller', 'card', 'disk', 'bus', 'ide']\n",
      "Topic 1: ['like', 'use', 'get', 'one', 'would', 'time', 'problem', 'please', 'could']\n",
      "Topic 2: ['edu', 'writes', 'com', 'article', 'wrote', 'livesey', 'matthew', 'keith', 'alt']\n",
      "Topic 3: ['god', 'say', 'people', 'believe', 'religion', 'would', 'atheists', 'atheism', 'thing']\n",
      "\n",
      "Topic 0: ['car', 'drivers', 'buphy', 'livesey', 'uiuc', 'bobbe', 'mathew', 'bus', 'irq']\n",
      "Topic 1: ['religion', 'religions', 'atheism', 'atheists', 'atheist', 'anybody', 'religious', 'islam', 'faith']\n",
      "Topic 2: ['science', 'edu', 'physics', 'scientific', 'com', 'california', 'beauchaine', 'theology', 'kevin']\n",
      "Topic 3: ['hardware', 'disk', 'disks', 'server', 'drives', 'cpu', 'software', 'config', 'motherboard']\n"
     ]
    }
   ],
   "source": [
    "get_topic_words_etm(etm_path5)\n",
    "print()\n",
    "get_topic_words_etm(etm_path6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
